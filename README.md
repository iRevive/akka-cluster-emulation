### Summary
При запуске системы достаточно указать адрес 1 активной ноды. При подключении к ней
активная нода оповестит все остальные о подключении нового элемента в сеть.
После запуска происходит оповещение активных нод из конфигурации о старте приложения, 
также отправляется сообщение с новым значением интервала отправки сообщений.
При падении удаленной ноды, система сделает 10 попыток подключения с задержкой в 5 секунд.
При полном отключении от сети нода продолжит функционировать. 
После появления подключения, система будет корректно работать когда к ней подключатся другие ноды.

### Architecture Overview
Работа системы завязана на двух основых акторах: Worker и NodeWatcher.
Worker - создается при запуске приложения. После запуска оповещает ноды из конфигуации о своём старте:
отправляет сообщение NewNode и SetTimeout активным нода.
Для каждой новой ноды создает NodeWatcher. 
При получении NodeMessage & SetTimeout проксирует их всем детям(NodeWatcher) (кроме отправителя), которые отправляют это сообщение удалённым нодам.

NodeWatcher - следит за состоянием удалённой ноды. При падении соединения, пытается восстановить его через определённые интервалы.
Также занимается проксированием сообщений удалённой ноде.

Приблизительная структура:

             Guardian System Actor
                       |
                    Worker
      /                |              \    
    NodeWatcher A    NodeWatcher B    NodeWatcher C


### Throughput calculation
После запуска новой ноды отправляется 2 сообщения NewNode и SetTimeout. После получения SetTimeout, через 2 секунды, происходит
пересчет производительности. Задержка необходима для того, что бы рассчет происходил в самом актуальном состоянии.
Рассчет происходит по формуле: (Active Nodes / 100) * Messages Per Second.
Полученное значение можно считать коэффициентом пропускной способности.

Кластер представляет собой полный граф, т.к. каждая нода знает о других нодах.
В таком случае, достаточно оценить производительность одной ноды, для других оценка будет приблизительно 
такой же (при условии, что инстансы развернуты на одинаковых по мощностям серверах). 
В моей реализации используется очень грубая оценка - количество нод / количество сообщений в секунду.
Для более точной реализации и прогнозирования производительности, лучше использовать распределение Пуассона из ТМО. 
В таком случае пределом системы будет перелом на графике «количество нод / количество сообщений в секунду».

### Known issues
Логгирование ошибок `Association with remote system ...` у remote модуля отключить нельзя. 
https://github.com/akka/akka/issues/16615

### Configuration
Хост и порт по которым нода будет доступна в сети:
```hocon
akka.netty.tcp {
  hostname = "127.0.0.1"
  port = 60001
}
```

Конфигурация активных нод:
```hocon
cluster.nodes = [
	{hostname = "localhost", port = 60002},
	{hostname = "localhost", port = 60003}
]

cluster.message-interval = 100ms
```

### Configuration example
В папке `dist` находится уже собранное приложение с тестовыми скриптами и конфигурациями.
Для запуска каждой ноды достаточно выполнить `sh node1.sh`, `sh node2.sh`, `sh node3.sh`.
Для остановки приложения достаточно закрыть окно терминала.

### Deploy
Выполнить `sbt stage` в корневой директории проекта. Собранный проект будет доступен в `target/universal/stage`.

### Node start
В папке `target/universal/stage` выполнить `sh bin/akka-cluster-emulation -Dconfig.file=PathToConfig` (для Unix систем)   
или `bin/akka-cluster-emulation.bat -Dconfig.file=PathToConfig` (для Windows).

### Тонкие места
* Механизм фильтрации обработанных сообщений. Он реализован с помощью хэш карты и очищается по заданному таймауту. 
При существенном увеличении количества сообщений это может стать бутылочным горлышком системы.
* Если будет возникать ситуация с полным потребление ресурсов CPU, можно воспользоваться разными диспатчерами. 
Один использовать для обработки внешних сообщений, другой для внутренних. 
Такой подход обеспечит частичное функционирование системы.